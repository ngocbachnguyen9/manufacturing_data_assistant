#!/usr/bin/env python3
"""
Visualization Guide for Phase 5 Analysis

This script provides descriptions and insights for all generated visualizations
in the Phase 5 human vs LLM comparative analysis.
"""

import sys
from pathlib import Path

def display_visualization_guide(viz_dir: str = "experiments/phase5_analysis/visualizations"):
    """Display comprehensive guide to all Phase 5 visualizations"""
    
    viz_path = Path(viz_dir)
    
    if not viz_path.exists():
        print(f"‚ùå Visualization directory not found: {viz_path}")
        print("Please run Phase 5 analysis first: python run_phase5_analysis.py")
        return
    
    print("üìä PHASE 5 VISUALIZATION GUIDE")
    print("=" * 80)
    print("This guide explains all visualizations generated by the Phase 5 analysis.")
    print("Each visualization provides unique insights into human vs LLM performance.")
    print()
    
    # Define visualization descriptions
    visualizations = {
        "enhanced_model_comparison_dashboard.png": {
            "title": "üéØ Enhanced Model Comparison Dashboard",
            "description": "Comprehensive overview of all model performance metrics",
            "components": [
                "‚Ä¢ Accuracy vs Speed scatter plot with statistical significance bubbles",
                "‚Ä¢ Model accuracy ranking with human baseline comparison", 
                "‚Ä¢ Speed improvement ranking across all models",
                "‚Ä¢ Statistical significance matrix (accuracy & time tests)",
                "‚Ä¢ Performance radar chart for top 3 models",
                "‚Ä¢ Summary table with recommendations for each model"
            ],
            "insights": [
                "Identifies best overall performers balancing accuracy and speed",
                "Shows which models have statistically significant improvements",
                "Provides clear recommendations for different use cases"
            ]
        },
        
        "complexity_performance_matrix.png": {
            "title": "üß© Task Complexity Performance Matrix", 
            "description": "Detailed analysis of model performance across easy/medium/hard tasks",
            "components": [
                "‚Ä¢ Accuracy heatmap by model and complexity level",
                "‚Ä¢ Human vs model comparison across complexity levels",
                "‚Ä¢ Accuracy improvement heatmap over human baseline",
                "‚Ä¢ Speed improvement matrix by complexity",
                "‚Ä¢ Best model identification for each complexity level",
                "‚Ä¢ Analysis of models struggling with specific complexities"
            ],
            "insights": [
                "Reveals which models excel at different task difficulties",
                "Identifies complexity-specific strengths and weaknesses",
                "Shows if models maintain performance across difficulty levels"
            ]
        },
        
        "quality_robustness_analysis.png": {
            "title": "üõ°Ô∏è Data Quality Robustness Analysis",
            "description": "Comprehensive analysis of model performance under data corruption",
            "components": [
                "‚Ä¢ Accuracy heatmap across Q0-Q3 quality conditions",
                "‚Ä¢ Robustness score ranking (corrupted vs perfect data ratio)",
                "‚Ä¢ Quality degradation pattern analysis",
                "‚Ä¢ Perfect vs corrupted data performance scatter plot",
                "‚Ä¢ Best model identification for each quality condition",
                "‚Ä¢ Quality impact analysis and robustness vs accuracy trade-offs"
            ],
            "insights": [
                "Identifies most robust models for poor data quality environments",
                "Shows how different corruption types affect each model",
                "Reveals trade-offs between accuracy and robustness"
            ]
        },
        
        "model_specific_comparison.png": {
            "title": "üîç Model-Specific Performance Comparison",
            "description": "Individual model analysis with statistical validation",
            "components": [
                "‚Ä¢ Accuracy comparison with human baseline indicators",
                "‚Ä¢ Speed improvement factors for each model",
                "‚Ä¢ Statistical significance heatmap",
                "‚Ä¢ Accuracy difference from human baseline"
            ],
            "insights": [
                "Provides detailed view of each model's strengths",
                "Shows statistical reliability of performance differences",
                "Identifies models that significantly outperform humans"
            ]
        },
        
        "model_complexity_analysis.png": {
            "title": "üìà Model Complexity Analysis",
            "description": "Model performance breakdown by task complexity",
            "components": [
                "‚Ä¢ Accuracy heatmap by model and complexity",
                "‚Ä¢ Accuracy improvement over humans by complexity",
                "‚Ä¢ Speed improvement analysis by complexity",
                "‚Ä¢ Best model identification for each complexity level"
            ],
            "insights": [
                "Shows which models handle complex tasks better",
                "Reveals complexity-specific performance patterns",
                "Identifies optimal models for different task difficulties"
            ]
        },
        
        "model_quality_analysis.png": {
            "title": "üîç Model Quality Analysis",
            "description": "Model performance across data quality conditions",
            "components": [
                "‚Ä¢ Accuracy heatmap across quality conditions",
                "‚Ä¢ Robustness analysis and degradation patterns",
                "‚Ä¢ Quality-specific performance comparisons",
                "‚Ä¢ Best model identification by quality condition"
            ],
            "insights": [
                "Identifies models best suited for corrupted data",
                "Shows how data quality affects each model differently",
                "Reveals most reliable models across quality conditions"
            ]
        },
        
        "overall_performance_comparison.png": {
            "title": "üìä Overall Performance Comparison",
            "description": "High-level human vs LLM performance overview",
            "components": [
                "‚Ä¢ Overall accuracy comparison",
                "‚Ä¢ Average completion time comparison", 
                "‚Ä¢ Cost per task analysis",
                "‚Ä¢ Efficiency metrics (speedup and cost ratios)"
            ],
            "insights": [
                "Provides executive summary of LLM vs human performance",
                "Shows overall cost-effectiveness of LLM deployment",
                "Demonstrates speed and efficiency improvements"
            ]
        },
        
        "performance_by_complexity.png": {
            "title": "üéØ Performance by Complexity",
            "description": "Aggregated performance analysis across task complexity levels",
            "components": [
                "‚Ä¢ Accuracy comparison by complexity (human vs LLM average)",
                "‚Ä¢ Time comparison by complexity level"
            ],
            "insights": [
                "Shows how complexity affects human vs LLM performance",
                "Identifies complexity levels where LLMs excel most",
                "Reveals performance gaps across difficulty levels"
            ]
        },
        
        "performance_by_quality.png": {
            "title": "üîç Performance by Quality",
            "description": "Aggregated performance analysis across data quality conditions",
            "components": [
                "‚Ä¢ Accuracy comparison by quality condition",
                "‚Ä¢ Time comparison across quality levels"
            ],
            "insights": [
                "Shows impact of data quality on performance",
                "Demonstrates LLM robustness to data corruption",
                "Identifies quality conditions where LLMs struggle"
            ]
        },
        
        "time_vs_accuracy_scatter.png": {
            "title": "‚ö° Time vs Accuracy Trade-off",
            "description": "Analysis of speed vs accuracy trade-offs",
            "components": [
                "‚Ä¢ Scatter plot of completion time vs accuracy",
                "‚Ä¢ Human vs LLM performance points",
                "‚Ä¢ Trend lines showing performance relationships"
            ],
            "insights": [
                "Reveals trade-offs between speed and accuracy",
                "Shows optimal performance regions",
                "Identifies models with best speed-accuracy balance"
            ]
        },
        
        "cost_analysis.png": {
            "title": "üí∞ Cost Analysis",
            "description": "Cost-effectiveness comparison between humans and LLMs",
            "components": [
                "‚Ä¢ Cost per task comparison",
                "‚Ä¢ Cost per correct answer analysis",
                "‚Ä¢ Time vs cost relationship",
                "‚Ä¢ Return on investment calculations"
            ],
            "insights": [
                "Demonstrates economic benefits of LLM deployment",
                "Shows cost efficiency across different metrics",
                "Provides ROI analysis for decision making"
            ]
        }
    }
    
    # Display guide for each visualization
    for filename, info in visualizations.items():
        viz_file = viz_path / filename
        
        if viz_file.exists():
            print(f"{info['title']}")
            print("‚îÄ" * 60)
            print(f"üìÅ File: {filename}")
            print(f"üìù Description: {info['description']}")
            print()
            
            print("üîß Components:")
            for component in info['components']:
                print(f"   {component}")
            print()
            
            print("üí° Key Insights:")
            for insight in info['insights']:
                print(f"   ‚Ä¢ {insight}")
            print()
            print("=" * 80)
            print()
        else:
            print(f"‚ö†Ô∏è  {info['title']} - File not found: {filename}")
            print()
    
    # Usage recommendations
    print("üöÄ VISUALIZATION USAGE RECOMMENDATIONS")
    print("=" * 80)
    print()
    
    print("üìã FOR EXECUTIVE PRESENTATIONS:")
    print("   ‚Ä¢ enhanced_model_comparison_dashboard.png - Complete overview")
    print("   ‚Ä¢ overall_performance_comparison.png - High-level summary")
    print("   ‚Ä¢ cost_analysis.png - Economic justification")
    print()
    
    print("üî¨ FOR TECHNICAL ANALYSIS:")
    print("   ‚Ä¢ complexity_performance_matrix.png - Detailed complexity analysis")
    print("   ‚Ä¢ quality_robustness_analysis.png - Data quality robustness")
    print("   ‚Ä¢ model_specific_comparison.png - Individual model evaluation")
    print()
    
    print("üéØ FOR SPECIFIC USE CASES:")
    print("   ‚Ä¢ complexity_performance_matrix.png - Task difficulty requirements")
    print("   ‚Ä¢ quality_robustness_analysis.png - Poor data quality environments")
    print("   ‚Ä¢ time_vs_accuracy_scatter.png - Speed vs accuracy trade-offs")
    print()
    
    print("üìä FOR RESEARCH PUBLICATIONS:")
    print("   ‚Ä¢ All visualizations provide publication-quality figures")
    print("   ‚Ä¢ Statistical significance clearly indicated")
    print("   ‚Ä¢ Comprehensive methodology validation")
    print()
    
    print("üí° QUICK ACCESS COMMANDS:")
    print("   ‚Ä¢ View all results: ls experiments/phase5_analysis/visualizations/")
    print("   ‚Ä¢ Open specific chart: open experiments/phase5_analysis/visualizations/[filename]")
    print("   ‚Ä¢ Generate new analysis: python run_phase5_analysis.py")
    print("   ‚Ä¢ Get model rankings: python experiments/phase5_analysis/model_ranking_summary.py")
    print("   ‚Ä¢ Get quick insights: python experiments/phase5_analysis/quick_insights.py")
    print()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        display_visualization_guide(sys.argv[1])
    else:
        display_visualization_guide()
