#!/usr/bin/env python3
"""
Visualization Guide for Phase 5 Analysis

This script provides descriptions and insights for all generated visualizations
in the Phase 5 human vs LLM comparative analysis.
"""

import sys
from pathlib import Path

def display_visualization_guide(viz_dir: str = "experiments/phase5_analysis/visualizations"):
    """Display comprehensive guide to all Phase 5 visualizations"""
    
    viz_path = Path(viz_dir)
    
    if not viz_path.exists():
        print(f"âŒ Visualization directory not found: {viz_path}")
        print("Please run Phase 5 analysis first: python run_phase5_analysis.py")
        return
    
    print("ðŸ“Š PHASE 5 VISUALIZATION GUIDE")
    print("=" * 80)
    print("This guide explains all visualizations generated by the Phase 5 analysis.")
    print("Each visualization provides unique insights into human vs LLM performance.")
    print()
    
    # Define visualization descriptions
    visualizations = {
        "enhanced_model_comparison_dashboard.png": {
            "title": "ðŸŽ¯ Enhanced Model Comparison Dashboard",
            "description": "Comprehensive overview of all model performance metrics",
            "components": [
                "â€¢ Accuracy vs Speed scatter plot with statistical significance bubbles",
                "â€¢ Model accuracy ranking with human baseline comparison", 
                "â€¢ Speed improvement ranking across all models",
                "â€¢ Statistical significance matrix (accuracy & time tests)",
                "â€¢ Performance radar chart for top 3 models",
                "â€¢ Summary table with recommendations for each model"
            ],
            "insights": [
                "Identifies best overall performers balancing accuracy and speed",
                "Shows which models have statistically significant improvements",
                "Provides clear recommendations for different use cases"
            ]
        },
        
        "complexity_performance_matrix.png": {
            "title": "ðŸ§© Task Complexity Performance Matrix", 
            "description": "Detailed analysis of model performance across easy/medium/hard tasks",
            "components": [
                "â€¢ Accuracy heatmap by model and complexity level",
                "â€¢ Human vs model comparison across complexity levels",
                "â€¢ Accuracy improvement heatmap over human baseline",
                "â€¢ Speed improvement matrix by complexity",
                "â€¢ Best model identification for each complexity level",
                "â€¢ Analysis of models struggling with specific complexities"
            ],
            "insights": [
                "Reveals which models excel at different task difficulties",
                "Identifies complexity-specific strengths and weaknesses",
                "Shows if models maintain performance across difficulty levels"
            ]
        },
        
        "quality_robustness_analysis.png": {
            "title": "ðŸ›¡ï¸ Data Quality Robustness Analysis",
            "description": "Comprehensive analysis of model performance under data corruption",
            "components": [
                "â€¢ Accuracy heatmap across Q0-Q3 quality conditions",
                "â€¢ Robustness score ranking (corrupted vs perfect data ratio)",
                "â€¢ Quality degradation pattern analysis",
                "â€¢ Perfect vs corrupted data performance scatter plot",
                "â€¢ Best model identification for each quality condition",
                "â€¢ Quality impact analysis and robustness vs accuracy trade-offs"
            ],
            "insights": [
                "Identifies most robust models for poor data quality environments",
                "Shows how different corruption types affect each model",
                "Reveals trade-offs between accuracy and robustness"
            ]
        },
        
        "model_specific_comparison.png": {
            "title": "ðŸ” Model-Specific Performance Comparison",
            "description": "Individual model analysis with statistical validation",
            "components": [
                "â€¢ Accuracy comparison with human baseline indicators",
                "â€¢ Speed improvement factors for each model",
                "â€¢ Statistical significance heatmap",
                "â€¢ Accuracy difference from human baseline"
            ],
            "insights": [
                "Provides detailed view of each model's strengths",
                "Shows statistical reliability of performance differences",
                "Identifies models that significantly outperform humans"
            ]
        },
        
        "model_complexity_analysis.png": {
            "title": "ðŸ“ˆ Model Complexity Analysis",
            "description": "Model performance breakdown by task complexity",
            "components": [
                "â€¢ Accuracy heatmap by model and complexity",
                "â€¢ Accuracy improvement over humans by complexity",
                "â€¢ Speed improvement analysis by complexity",
                "â€¢ Best model identification for each complexity level"
            ],
            "insights": [
                "Shows which models handle complex tasks better",
                "Reveals complexity-specific performance patterns",
                "Identifies optimal models for different task difficulties"
            ]
        },
        
        "model_quality_analysis.png": {
            "title": "ðŸ” Model Quality Analysis",
            "description": "Model performance across data quality conditions",
            "components": [
                "â€¢ Accuracy heatmap across quality conditions",
                "â€¢ Robustness analysis and degradation patterns",
                "â€¢ Quality-specific performance comparisons",
                "â€¢ Best model identification by quality condition"
            ],
            "insights": [
                "Identifies models best suited for corrupted data",
                "Shows how data quality affects each model differently",
                "Reveals most reliable models across quality conditions"
            ]
        },
        
        "overall_performance_comparison.png": {
            "title": "ðŸ“Š Overall Performance Comparison",
            "description": "High-level human vs LLM performance overview",
            "components": [
                "â€¢ Overall accuracy comparison",
                "â€¢ Average completion time comparison", 
                "â€¢ Cost per task analysis",
                "â€¢ Efficiency metrics (speedup and cost ratios)"
            ],
            "insights": [
                "Provides executive summary of LLM vs human performance",
                "Shows overall cost-effectiveness of LLM deployment",
                "Demonstrates speed and efficiency improvements"
            ]
        },
        
        "performance_by_complexity.png": {
            "title": "ðŸŽ¯ Performance by Complexity",
            "description": "Aggregated performance analysis across task complexity levels",
            "components": [
                "â€¢ Accuracy comparison by complexity (human vs LLM average)",
                "â€¢ Time comparison by complexity level"
            ],
            "insights": [
                "Shows how complexity affects human vs LLM performance",
                "Identifies complexity levels where LLMs excel most",
                "Reveals performance gaps across difficulty levels"
            ]
        },
        
        "performance_by_quality.png": {
            "title": "ðŸ” Performance by Quality",
            "description": "Aggregated performance analysis across data quality conditions",
            "components": [
                "â€¢ Accuracy comparison by quality condition",
                "â€¢ Time comparison across quality levels"
            ],
            "insights": [
                "Shows impact of data quality on performance",
                "Demonstrates LLM robustness to data corruption",
                "Identifies quality conditions where LLMs struggle"
            ]
        },
        
        "time_vs_accuracy_scatter.png": {
            "title": "âš¡ Time vs Accuracy Trade-off",
            "description": "Analysis of speed vs accuracy trade-offs",
            "components": [
                "â€¢ Scatter plot of completion time vs accuracy",
                "â€¢ Human vs LLM performance points",
                "â€¢ Trend lines showing performance relationships"
            ],
            "insights": [
                "Reveals trade-offs between speed and accuracy",
                "Shows optimal performance regions",
                "Identifies models with best speed-accuracy balance"
            ]
        },
        
        "cost_analysis.png": {
            "title": "ðŸ’° Cost Analysis",
            "description": "Cost-effectiveness comparison between humans and LLMs",
            "components": [
                "â€¢ Cost per task comparison",
                "â€¢ Cost per correct answer analysis",
                "â€¢ Time vs cost relationship",
                "â€¢ Return on investment calculations"
            ],
            "insights": [
                "Demonstrates economic benefits of LLM deployment",
                "Shows cost efficiency across different metrics",
                "Provides ROI analysis for decision making"
            ]
        }
    }
    
    # Display guide for each visualization
    for filename, info in visualizations.items():
        viz_file = viz_path / filename
        
        if viz_file.exists():
            print(f"{info['title']}")
            print("â”€" * 60)
            print(f"ðŸ“ File: {filename}")
            print(f"ðŸ“ Description: {info['description']}")
            print()
            
            print("ðŸ”§ Components:")
            for component in info['components']:
                print(f"   {component}")
            print()
            
            print("ðŸ’¡ Key Insights:")
            for insight in info['insights']:
                print(f"   â€¢ {insight}")
            print()
            print("=" * 80)
            print()
        else:
            print(f"âš ï¸  {info['title']} - File not found: {filename}")
            print()
    
    # Usage recommendations
    print("ðŸš€ VISUALIZATION USAGE RECOMMENDATIONS")
    print("=" * 80)
    print()
    
    print("ðŸ“‹ FOR EXECUTIVE PRESENTATIONS:")
    print("   â€¢ enhanced_model_comparison_dashboard.png - Complete overview")
    print("   â€¢ overall_performance_comparison.png - High-level summary")
    print("   â€¢ cost_analysis.png - Economic justification")
    print()
    
    print("ðŸ”¬ FOR TECHNICAL ANALYSIS:")
    print("   â€¢ complexity_performance_matrix.png - Detailed complexity analysis")
    print("   â€¢ quality_robustness_analysis.png - Data quality robustness")
    print("   â€¢ model_specific_comparison.png - Individual model evaluation")
    print()
    
    print("ðŸŽ¯ FOR SPECIFIC USE CASES:")
    print("   â€¢ complexity_performance_matrix.png - Task difficulty requirements")
    print("   â€¢ quality_robustness_analysis.png - Poor data quality environments")
    print("   â€¢ time_vs_accuracy_scatter.png - Speed vs accuracy trade-offs")
    print()
    
    print("ðŸ“Š FOR RESEARCH PUBLICATIONS:")
    print("   â€¢ All visualizations provide publication-quality figures")
    print("   â€¢ Statistical significance clearly indicated")
    print("   â€¢ Comprehensive methodology validation")
    print()
    
    print("ðŸ’¡ QUICK ACCESS COMMANDS:")
    print("   â€¢ View all results: ls experiments/phase5_analysis/visualizations/")
    print("   â€¢ Open specific chart: open experiments/phase5_analysis/visualizations/[filename]")
    print("   â€¢ Generate new analysis: python run_phase5_analysis.py")
    print("   â€¢ Get model rankings: python experiments/phase5_analysis/model_ranking_summary.py")
    print("   â€¢ Get quick insights: python experiments/phase5_analysis/quick_insights.py")
    print()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        display_visualization_guide(sys.argv[1])
    else:
        display_visualization_guide()
